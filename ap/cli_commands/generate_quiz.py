import yaml
import typer
import asyncio
import os
import re
import time
import random
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from ap.core.concept_map import ConceptMap, slugify
from ap.core.utils import call_deepseek_with_retry
from ap.core.settings import WORKSPACE_DIR
from ap.cli_commands.explain import analyze_document_structure


@dataclass
class ContentChunk:
    """å†…å®¹å—"""
    title: str
    content: str
    target_questions: int
    chunk_id: int


@dataclass
class GenerationResult:
    """ç”Ÿæˆç»“æœ"""
    chunk_id: int
    questions: List[Dict[str, Any]]
    generation_time: float
    error: str = None


class ParallelQuizGenerator:
    """å¹¶è¡Œæµ‹è¯•é¢˜ç”Ÿæˆå™¨"""
    
    def __init__(self, max_concurrent: int = 6):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸º6
        """
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)

    def extract_keywords(self, text: str) -> set:
        """
        ä»é¢˜ç›®æ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: é¢˜ç›®æ–‡æœ¬
            
        Returns:
            å…³é”®è¯é›†åˆ
        """
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œè½¬æ¢ä¸ºå°å†™
        cleaned_text = re.sub(r'[^\w\s]', '', text.lower())
        # åˆ†è¯å¹¶è¿‡æ»¤çŸ­è¯
        words = [word for word in cleaned_text.split() if len(word) > 1]
        return set(words)

    def calculate_similarity(self, question1: Dict[str, Any], question2: Dict[str, Any]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªé¢˜ç›®çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨Jaccardç›¸ä¼¼åº¦ï¼‰
        
        Args:
            question1: ç¬¬ä¸€ä¸ªé¢˜ç›®
            question2: ç¬¬äºŒä¸ªé¢˜ç›®
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        text1 = question1.get('question', '')
        text2 = question2.get('question', '')
        
        keywords1 = self.extract_keywords(text1)
        keywords2 = self.extract_keywords(text2)
        
        if not keywords1 and not keywords2:
            return 0.0
        
        intersection = keywords1.intersection(keywords2)
        union = keywords1.union(keywords2)
        
        return len(intersection) / len(union) if union else 0.0

    def remove_duplicate_questions(self, questions: List[Dict[str, Any]], 
                                 similarity_threshold: float = 0.6) -> List[Dict[str, Any]]:
        """
        ç§»é™¤é‡å¤çš„é¢˜ç›®
        
        Args:
            questions: é¢˜ç›®åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯é‡å¤é¢˜ç›®
            
        Returns:
            å»é‡åçš„é¢˜ç›®åˆ—è¡¨
        """
        if not questions:
            return []
        
        unique_questions = []
        
        for current_question in questions:
            is_duplicate = False
            
            for existing_question in unique_questions:
                similarity = self.calculate_similarity(current_question, existing_question)
                if similarity > similarity_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_questions.append(current_question)
        
        return unique_questions

    def create_chunk_prompt(self, chunk: ContentChunk, concept_name: str) -> str:
        """ä¸ºå†…å®¹å—åˆ›å»ºç”Ÿæˆæç¤º"""
        return f"""åŸºäºä»¥ä¸‹å†…å®¹ï¼Œä¸ºæ¦‚å¿µ "{concept_name}" çš„ "{chunk.title}" éƒ¨åˆ†ç”Ÿæˆ {chunk.target_questions} é“é«˜è´¨é‡çš„é€‰æ‹©é¢˜ã€‚

å†…å®¹ï¼š
{chunk.content}

è¦æ±‚ï¼š
1. é¢˜ç›®åº”è¦†ç›–è¿™éƒ¨åˆ†å†…å®¹çš„å…³é”®çŸ¥è¯†ç‚¹
2. æ¯é“é¢˜æœ‰4ä¸ªé€‰é¡¹ï¼ˆAã€Bã€Cã€Dï¼‰
3. åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆ
4. é€‰é¡¹åˆ†å¸ƒè¦ç›¸å¯¹å‡åŒ€
5. é¢˜ç›®éš¾åº¦é€‚ä¸­ï¼Œé€‚åˆåˆå­¦è€…
6. ä½¿ç”¨ä¸­æ–‡

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹YAMLæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•ä»£ç å—æ ‡è®°ï¼š

- question: "é¢˜ç›®å†…å®¹"
  options:
    A: "é€‰é¡¹Aå†…å®¹"
    B: "é€‰é¡¹Bå†…å®¹"
    C: "é€‰é¡¹Cå†…å®¹"
    D: "é€‰é¡¹Då†…å®¹"
  answer: "A"
  explanation: "ç­”æ¡ˆè§£é‡Šå†…å®¹"

ç”Ÿæˆ {chunk.target_questions} é“é¢˜ç›®ï¼š"""

    async def generate_chunk_questions(self, chunk: ContentChunk, concept_name: str) -> GenerationResult:
        """ä¸ºå•ä¸ªå†…å®¹å—ç”Ÿæˆé¢˜ç›®"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            start_time = time.time()
            
            try:
                prompt = self.create_chunk_prompt(chunk, concept_name)
                
                # ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„APIè°ƒç”¨å‡½æ•°
                def retry_callback(attempt, max_retries):
                    print(f"   å— {chunk.chunk_id}: ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯•...")
                
                # å°†åŒæ­¥è°ƒç”¨åŒ…è£…åœ¨å¼‚æ­¥æ‰§è¡Œä¸­
                content = await asyncio.to_thread(
                    call_deepseek_with_retry,
                    messages=prompt,
                    model="deepseek-chat",
                    max_retries=3,
                    base_temperature=0.3,
                    max_tokens=4096,
                    retry_callback=retry_callback
                )
                
                # è§£æYAML
                questions = yaml.safe_load(content)
                
                # éªŒè¯æ ¼å¼
                if not isinstance(questions, list):
                    raise ValueError(f"å— {chunk.chunk_id} ç”Ÿæˆçš„å†…å®¹ä¸æ˜¯åˆ—è¡¨æ ¼å¼")
                
                # éªŒè¯æ¯ä¸ªé¢˜ç›®
                for i, q in enumerate(questions):
                    if not all(key in q for key in ['question', 'options', 'answer', 'explanation']):
                        raise ValueError(f"å— {chunk.chunk_id} ç¬¬ {i+1} é¢˜æ ¼å¼ä¸å®Œæ•´")
                
                generation_time = time.time() - start_time
                
                return GenerationResult(
                    chunk_id=chunk.chunk_id,
                    questions=questions,
                    generation_time=generation_time
                )
                
            except Exception as e:
                generation_time = time.time() - start_time
                return GenerationResult(
                    chunk_id=chunk.chunk_id,
                    questions=[],
                    generation_time=generation_time,
                    error=str(e)
                )

    def merge_and_optimize_results(self, results: List[GenerationResult]) -> List[Dict[str, Any]]:
        """åˆå¹¶ç»“æœå¹¶ä¼˜åŒ–ç­”æ¡ˆåˆ†å¸ƒ"""
        all_questions = []
        
        # åˆå¹¶æ‰€æœ‰æˆåŠŸç”Ÿæˆçš„é¢˜ç›®
        for result in results:
            if result.error is None:
                all_questions.extend(result.questions)
        
        if not all_questions:
            raise ValueError("æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•é¢˜ç›®")
        
        # å»é‡å¤„ç†
        all_questions = self.remove_duplicate_questions(all_questions)
        
        # åˆ†æç­”æ¡ˆåˆ†å¸ƒ
        answer_counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        for q in all_questions:
            answer = q.get('answer', '')
            if answer in answer_counts:
                answer_counts[answer] += 1
        
        # å¦‚æœåˆ†å¸ƒä¸å‡åŒ€ï¼Œè¿›è¡Œè°ƒæ•´
        total_questions = len(all_questions)
        target_per_option = total_questions / 4
        
        # æ‰¾å‡ºéœ€è¦è°ƒæ•´çš„é¢˜ç›®
        adjustments_needed = {}
        for option, count in answer_counts.items():
            diff = count - target_per_option
            if abs(diff) > 1:  # å…è®¸1é¢˜çš„è¯¯å·®
                adjustments_needed[option] = diff
        
        # ç®€å•çš„ç­”æ¡ˆé‡æ–°åˆ†é…ï¼ˆä¿æŒé¢˜ç›®å†…å®¹ä¸å˜ï¼Œåªè°ƒæ•´ç­”æ¡ˆï¼‰
        if adjustments_needed:
            self._rebalance_answers(all_questions, adjustments_needed)
        
        return all_questions

    def _rebalance_answers(self, questions: List[Dict[str, Any]], adjustments: Dict[str, float]):
        """é‡æ–°å¹³è¡¡ç­”æ¡ˆåˆ†å¸ƒ"""
        # æ‰¾å‡ºè¿‡å¤šå’Œè¿‡å°‘çš„é€‰é¡¹
        excess_options = [opt for opt, diff in adjustments.items() if diff > 1]
        deficit_options = [opt for opt, diff in adjustments.items() if diff < -1]
        
        if not excess_options or not deficit_options:
            return
        
        # ç®€å•ç­–ç•¥ï¼šéšæœºé‡æ–°åˆ†é…ä¸€äº›é¢˜ç›®çš„ç­”æ¡ˆ
        for i, question in enumerate(questions):
            current_answer = question.get('answer', '')
            
            # å¦‚æœå½“å‰ç­”æ¡ˆæ˜¯è¿‡å¤šçš„é€‰é¡¹ï¼Œè€ƒè™‘æ”¹ä¸ºä¸è¶³çš„é€‰é¡¹
            if current_answer in excess_options and deficit_options:
                if random.random() < 0.3:  # 30%çš„æ¦‚ç‡è¿›è¡Œè°ƒæ•´
                    new_answer = random.choice(deficit_options)
                    
                    # äº¤æ¢é€‰é¡¹å†…å®¹
                    options = question['options']
                    if current_answer in options and new_answer in options:
                        # äº¤æ¢é€‰é¡¹å†…å®¹ï¼Œä½¿æ–°ç­”æ¡ˆæˆä¸ºæ­£ç¡®ç­”æ¡ˆ
                        options[current_answer], options[new_answer] = options[new_answer], options[current_answer]
                        question['answer'] = new_answer

    async def generate_parallel_quiz(self, concept_name: str, content: str, 
                                   target_questions: int = 10) -> Dict[str, Any]:
        """
        å¹¶è¡Œç”Ÿæˆæµ‹è¯•é¢˜
        
        Args:
            concept_name: æ¦‚å¿µåç§°
            content: å†…å®¹æ–‡æœ¬
            target_questions: ç›®æ ‡é¢˜ç›®æ•°é‡
            
        Returns:
            åŒ…å«é¢˜ç›®å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œç”Ÿæˆ '{concept_name}' çš„ {target_questions} é“æµ‹è¯•é¢˜")
        
        start_time = time.time()
        
        # æ ¹æ®é¢˜ç›®æ•°é‡å†³å®šåˆ†å—ç­–ç•¥
        if target_questions <= 5:
            # å°‘é‡é¢˜ç›®ï¼Œå•å—å¤„ç†
            chunks = [ContentChunk(
                title="å®Œæ•´å†…å®¹",
                content=content,
                target_questions=target_questions,
                chunk_id=0
            )]
        elif target_questions <= 12:
            # ä¸­ç­‰æ•°é‡ï¼Œåˆ†ä¸ºä¸¤å—
            chunk_size = target_questions // 2
            remaining = target_questions % 2
            
            # ç®€å•æŒ‰å†…å®¹é•¿åº¦åˆ†å‰²
            words = content.split()
            mid_point = len(words) // 2
            
            chunks = [
                ContentChunk(
                    title="å‰åŠéƒ¨åˆ†",
                    content=" ".join(words[:mid_point]),
                    target_questions=chunk_size + remaining,
                    chunk_id=0
                ),
                ContentChunk(
                    title="ååŠéƒ¨åˆ†", 
                    content=" ".join(words[mid_point:]),
                    target_questions=chunk_size,
                    chunk_id=1
                )
            ]
        else:
            # å¤§é‡é¢˜ç›®ï¼Œå¤šå—å¤„ç†ï¼ˆæ¯å—æœ€å¤š5é¢˜ï¼‰
            max_questions_per_chunk = 5
            num_chunks = (target_questions + max_questions_per_chunk - 1) // max_questions_per_chunk
            
            words = content.split()
            chunk_size = len(words) // num_chunks
            
            chunks = []
            for i in range(num_chunks):
                start_idx = i * chunk_size
                end_idx = start_idx + chunk_size if i < num_chunks - 1 else len(words)
                
                questions_for_chunk = min(max_questions_per_chunk, 
                                        target_questions - len(chunks) * max_questions_per_chunk)
                if i == num_chunks - 1:  # æœ€åä¸€å—åŒ…å«å‰©ä½™é¢˜ç›®
                    questions_for_chunk = target_questions - sum(c.target_questions for c in chunks)
                
                chunks.append(ContentChunk(
                    title=f"ç¬¬ {i+1} éƒ¨åˆ†",
                    content=" ".join(words[start_idx:end_idx]),
                    target_questions=questions_for_chunk,
                    chunk_id=i
                ))
        
        print(f"âœ‚ï¸  å†…å®¹å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
        for chunk in chunks:
            print(f"   å— {chunk.chunk_id + 1}: {chunk.title} ({chunk.target_questions} é¢˜)")
        
        # å¹¶è¡Œç”Ÿæˆ
        print(f"âš¡ å¼€å§‹å¹¶è¡Œç”Ÿæˆ (æœ€å¤§å¹¶å‘: {self.max_concurrent})...")
        
        tasks = [
            self.generate_chunk_questions(chunk, concept_name) 
            for chunk in chunks
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ å— {i} ç”Ÿæˆå¤±è´¥: {result}")
            else:
                valid_results.append(result)
                if result.error:
                    print(f"âŒ å— {result.chunk_id} ç”Ÿæˆå¤±è´¥: {result.error}")
                else:
                    print(f"âœ… å— {result.chunk_id} ç”ŸæˆæˆåŠŸ: {len(result.questions)} é¢˜ ({result.generation_time:.1f}s)")
        
        # åˆå¹¶å’Œä¼˜åŒ–
        print("ğŸ”„ åˆå¹¶ç»“æœå¹¶ä¼˜åŒ–ç­”æ¡ˆåˆ†å¸ƒ...")
        final_questions = self.merge_and_optimize_results(valid_results)
        
        total_time = time.time() - start_time
        
        # ç”ŸæˆæŠ¥å‘Š
        successful_chunks = len([r for r in valid_results if r.error is None])
        avg_chunk_time = sum(r.generation_time for r in valid_results if r.error is None) / max(1, successful_chunks)
        
        return {
            "questions": final_questions,
            "generation_stats": {
                "total_time": total_time,
                "target_questions": target_questions,
                "actual_questions": len(final_questions),
                "chunks_processed": len(chunks),
                "successful_chunks": successful_chunks,
                "average_chunk_time": avg_chunk_time,
                "parallel_efficiency": avg_chunk_time / total_time if total_time > 0 else 0
            }
        }


def create_quiz_prompt(concept: str, explanation_content: str,
                       num_questions: int) -> str:
    """æ„å»ºç”Ÿæˆæµ‹éªŒçš„ Prompt"""
    return f"""åŸºäºä»¥ä¸‹è§£é‡Šæ–‡æ¡£ï¼Œä¸ºæ¦‚å¿µ "{concept}" ç”Ÿæˆ {num_questions} é“é«˜è´¨é‡çš„é€‰æ‹©é¢˜ã€‚

è§£é‡Šæ–‡æ¡£å†…å®¹ï¼š
{explanation_content}

è¦æ±‚ï¼š
1. é¢˜ç›®åº”è¦†ç›–æ–‡æ¡£ä¸­çš„å…³é”®çŸ¥è¯†ç‚¹
2. æ¯é“é¢˜æœ‰4ä¸ªé€‰é¡¹ï¼ˆAã€Bã€Cã€Dï¼‰
3. åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆ
4. é€‰é¡¹åˆ†å¸ƒè¦å‡åŒ€ï¼ˆé¿å…æ‰€æœ‰ç­”æ¡ˆéƒ½æ˜¯Aæˆ–Bï¼‰
5. é¢˜ç›®éš¾åº¦é€‚ä¸­ï¼Œé€‚åˆåˆå­¦è€…
6. ä½¿ç”¨ä¸­æ–‡

**YAMLæ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š**
- ä½¿ç”¨2ä¸ªç©ºæ ¼ç¼©è¿›ï¼Œä¸è¦ä½¿ç”¨Tab
- æ‰€æœ‰æ–‡æœ¬å†…å®¹å¿…é¡»ç”¨åŒå¼•å·åŒ…å›´
- å¦‚æœæ–‡æœ¬åŒ…å«åŒå¼•å·ï¼Œè¯·ä½¿ç”¨å•å¼•å·åŒ…å›´æ•´ä¸ªæ–‡æœ¬
- æ¯ä¸ªé¢˜ç›®ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
- é€‰é¡¹å¿…é¡»ä¸¥æ ¼æŒ‰ç…§Aã€Bã€Cã€Dé¡ºåº
- answerå­—æ®µåªèƒ½æ˜¯"A"ã€"B"ã€"C"æˆ–"D"

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹YAMLæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•ä»£ç å—æ ‡è®°ï¼š

- question: "é¢˜ç›®å†…å®¹"
  options:
    A: "é€‰é¡¹Aå†…å®¹"
    B: "é€‰é¡¹Bå†…å®¹"
    C: "é€‰é¡¹Cå†…å®¹"
    D: "é€‰é¡¹Då†…å®¹"
  answer: "A"
  explanation: "ç­”æ¡ˆè§£é‡Šå†…å®¹"

- question: "ç¬¬äºŒé“é¢˜ç›®å†…å®¹"
  options:
    A: "é€‰é¡¹Aå†…å®¹"
    B: "é€‰é¡¹Bå†…å®¹"
    C: "é€‰é¡¹Cå†…å®¹"
    D: "é€‰é¡¹Då†…å®¹"
  answer: "B"
  explanation: "ç­”æ¡ˆè§£é‡Šå†…å®¹"

**é‡è¦æé†’ï¼š**
1. ç›´æ¥è¾“å‡ºYAMLå†…å®¹ï¼Œä¸è¦ä½¿ç”¨```yaml```ä»£ç å—åŒ…è£…
2. ç¡®ä¿æ¯ä¸ªå­—æ®µéƒ½æœ‰å€¼ï¼Œä¸è¦ç•™ç©º
3. æ‰€æœ‰å†’å·åé¢å¿…é¡»æœ‰ä¸€ä¸ªç©ºæ ¼
4. æ£€æŸ¥ç¼©è¿›æ˜¯å¦ä¸€è‡´ï¼ˆä½¿ç”¨2ä¸ªç©ºæ ¼ï¼‰
5. ç¡®ä¿æ²¡æœ‰å¤šä½™çš„ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦

ç”Ÿæˆ {num_questions} é“é¢˜ç›®ï¼š"""


def generate_quiz_internal(
    concept: str,
    **kwargs
):
    """
    åŸºäºè§£é‡Šæ–‡æ¡£ç”Ÿæˆæµ‹éªŒé¢˜ç›®ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰

    Args:
        concept: è¦ç”Ÿæˆæµ‹éªŒçš„æ¦‚å¿µåç§°
        **kwargs: é…ç½®å‚æ•°
            - num_questions: int = None, æŒ‡å®šé¢˜ç›®æ•°é‡ï¼ˆé»˜è®¤ä¸ºæ™ºèƒ½åˆ†æï¼‰
            - mode: str = "auto", ç”Ÿæˆæ¨¡å¼ï¼šautoï¼ˆæ™ºèƒ½åˆ†æï¼‰æˆ– fixedï¼ˆå›ºå®šæ¨¡å¼ï¼‰
            - max_tokens: int = 8192, æœ€å¤§è¾“å‡ºé•¿åº¦
            - use_parallel: bool = True, æ˜¯å¦ä½¿ç”¨å¹¶è¡Œç”Ÿæˆ
            - verbose: bool = False, æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    """
    # æå–å‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
    num_questions = kwargs.get('num_questions', None)
    mode = kwargs.get('mode', "auto")
    max_tokens = kwargs.get('max_tokens', 8192)
    use_parallel = kwargs.get('use_parallel', True)
    verbose = kwargs.get('verbose', False)
    
    if verbose:
        print(f"[GENERATE_QUIZ] å¼€å§‹ç”Ÿæˆæµ‹éªŒé¢˜ç›®: {concept}")
        print(f"[GENERATE_QUIZ] å‚æ•°: num_questions={num_questions}, mode={mode}, use_parallel={use_parallel}")
    try:
        if verbose:
            print(f"[GENERATE_QUIZ] åˆ›å»ºæ¦‚å¿µåœ°å›¾å®ä¾‹")
        # åˆ›å»ºæ¦‚å¿µåœ°å›¾å®ä¾‹
        concept_map = ConceptMap()

        # å¤„ç†æ¦‚å¿µåç§°
        if '/' in concept:
            topic_slug, concept_part = concept.split('/', 1)
            concept_slug = slugify(concept_part)
        else:
            concept_slug = slugify(concept)
            # å¦‚æœæ²¡æœ‰æä¾›ä¸»é¢˜ï¼Œåˆ™éœ€è¦æŸ¥æ‰¾
            topic_slug = concept_map.get_topic_by_concept(concept_slug)
            if not topic_slug:
                print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¦‚å¿µ '{concept}' æ‰€å±çš„ä¸»é¢˜ã€‚")
                return

        # æ„é€ è§£é‡Šæ–‡æ¡£è·¯å¾„
        explanation_dir = WORKSPACE_DIR / topic_slug / "explanation"
        explanation_file = explanation_dir / f"{concept_slug}.md"

        if not explanation_file.exists():
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è§£é‡Šæ–‡æ¡£ {explanation_file}")
            print("è¯·å…ˆè¿è¡Œ 'ap e' å‘½ä»¤ç”Ÿæˆè§£é‡Šæ–‡æ¡£")
            return

        # è¯»å–è§£é‡Šæ–‡æ¡£å†…å®¹
        with open(explanation_file, 'r', encoding='utf-8') as f:
            explanation_content = f.read()

        # æ™ºèƒ½åˆ†æé¢˜ç›®æ•°é‡
        if num_questions is None and mode == "auto":
            analysis = analyze_document_structure(explanation_content)
            num_questions = analysis['recommended_questions']
            print(
                f"ğŸ“Š æ™ºèƒ½åˆ†æ: å‘ç° {analysis['section_count']} ä¸ªä¸»è¦çŸ¥è¯†ç‚¹ï¼Œ"
                f"å»ºè®®ç”Ÿæˆ {num_questions} é“é¢˜ç›®"
            )

        # å¦‚æœä»ç„¶æ²¡æœ‰æŒ‡å®šæ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if num_questions is None:
            num_questions = 25

        # ç¡®ä¿æŒ‰ä¸»é¢˜ç»„ç»‡çš„ quizzes ç›®å½•å­˜åœ¨
        quizzes_dir = WORKSPACE_DIR / topic_slug / "quizzes"
        quizzes_dir.mkdir(parents=True, exist_ok=True)

        # æ„é€ è¾“å‡ºæ–‡ä»¶è·¯å¾„
        quiz_file = quizzes_dir / f"{concept_slug}.yml"

        # é€‰æ‹©ç”Ÿæˆç­–ç•¥
        if use_parallel and num_questions >= 5:
            print(f"ğŸš€ ä½¿ç”¨å¹¶è¡Œç”Ÿæˆç­–ç•¥")
            
            # ä½¿ç”¨å¹¶è¡Œç”Ÿæˆå™¨
            async def run_parallel_generation():
                generator = ParallelQuizGenerator(max_concurrent=6)
                
                result = await generator.generate_parallel_quiz(
                    concept_name=concept,
                    content=explanation_content,
                    target_questions=num_questions
                )
                return result
            
            # è¿è¡Œå¼‚æ­¥ç”Ÿæˆ
            result = asyncio.run(run_parallel_generation())
            quiz_data = result["questions"]
            
            # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            stats = result["generation_stats"]
            print(f"âš¡ å¹¶è¡Œç”Ÿæˆå®Œæˆ:")
            print(f"   æ€»è€—æ—¶: {stats['total_time']:.1f}ç§’")
            print(f"   ç”Ÿæˆé¢˜ç›®: {stats['actual_questions']}/{stats['target_questions']}")
            print(f"   å¹¶è¡Œæ•ˆç‡: {stats['parallel_efficiency']:.1%}")
            print(f"   æˆåŠŸå—æ•°: {stats['successful_chunks']}/{stats['chunks_processed']}")
            
        else:
            print(f"ğŸŒ ä½¿ç”¨ä¼ ç»Ÿå•çº¿ç¨‹ç”Ÿæˆ (é¢˜ç›®æ•°è¾ƒå°‘æˆ–ç¦ç”¨å¹¶è¡Œ)")
            
            # ä½¿ç”¨åŸæœ‰çš„å•çº¿ç¨‹ç”Ÿæˆé€»è¾‘
            quiz_content = call_deepseek_with_retry(
                messages=create_quiz_prompt(
                    concept, explanation_content, num_questions
                ),
                model="deepseek-chat",
                max_tokens=max_tokens,
                max_retries=3,
                base_temperature=0.5
            )

            # å°è¯•è§£æYAML
            quiz_data = yaml.safe_load(quiz_content)

        # éªŒè¯æ•°æ®ç»“æ„
        if not isinstance(quiz_data, list):
            error_type = type(quiz_data).__name__
            raise ValueError(f"ç”Ÿæˆçš„å†…å®¹ä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè€Œæ˜¯ {error_type}")

        if len(quiz_data) == 0:
            raise ValueError("ç”Ÿæˆçš„é¢˜ç›®åˆ—è¡¨ä¸ºç©º")

        # éªŒè¯æ¯ä¸ªé¢˜ç›®çš„ç»“æ„
        for i, question in enumerate(quiz_data):
            if not isinstance(question, dict):
                raise ValueError(f"ç¬¬ {i+1} é¢˜ä¸æ˜¯å­—å…¸æ ¼å¼")

            required_fields = ['question', 'options', 'answer', 'explanation']
            for field in required_fields:
                if field not in question:
                    raise ValueError(f"ç¬¬ {i+1} é¢˜ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

            # éªŒè¯é€‰é¡¹æ ¼å¼
            options = question.get('options', {})
            if not isinstance(options, dict):
                raise ValueError(f"ç¬¬ {i+1} é¢˜çš„é€‰é¡¹ä¸æ˜¯å­—å…¸æ ¼å¼")

            expected_options = ['A', 'B', 'C', 'D']
            missing_options = [opt for opt in expected_options
                               if opt not in options]
            if missing_options:
                options_str = ', '.join(missing_options)
                raise ValueError(f"ç¬¬ {i+1} é¢˜ç¼ºå°‘é€‰é¡¹: {options_str}")

            # éªŒè¯ç­”æ¡ˆæ ¼å¼
            answer = question.get('answer', '')
            if answer not in expected_options:
                raise ValueError(
                    f"ç¬¬ {i+1} é¢˜çš„ç­”æ¡ˆ '{answer}' ä¸åœ¨æœ‰æ•ˆé€‰é¡¹ä¸­"
                )

        print(f"âœ… YAMLæ ¼å¼æ­£ç¡®ï¼ŒæˆåŠŸç”Ÿæˆ {len(quiz_data)} é“é¢˜ç›®")

        # è§£æç”Ÿæˆçš„YAMLå†…å®¹è¿›è¡Œè´¨é‡æ£€æŸ¥
        try:
            # å¯¼å…¥è´¨é‡æ£€æŸ¥å™¨
            from ap.core.quiz_quality_checker import QuizQualityChecker
            quality_checker = QuizQualityChecker()

            # åˆ†æç­”æ¡ˆåˆ†å¸ƒ
            analysis_result = quality_checker.analyze_answer_distribution(
                quiz_data
            )

            if "error" not in analysis_result:
                quality_score = analysis_result.get('quality_score', 0)
                
                print(f"ğŸ¯ ç­”æ¡ˆåˆ†å¸ƒè´¨é‡æ£€æŸ¥:")
                distribution = analysis_result.get('distribution', {})
                for option, count in distribution.items():
                    percentage = (count / len(quiz_data)) * 100
                    print(f"   é€‰é¡¹ {option}: {count} é¢˜ ({percentage:.1f}%)")
                print(f"   è´¨é‡åˆ†æ•°: {quality_score:.1f}/100")

                # å¦‚æœè´¨é‡åˆ†æ•°ä½äº80ï¼Œè¿›è¡Œé™é»˜ç­”æ¡ˆéšæœºåŒ–
                if quality_score < 80:
                    print(f"ğŸ”„ è´¨é‡åˆ†æ•°åä½ï¼Œæ­£åœ¨ä¼˜åŒ–ç­”æ¡ˆåˆ†å¸ƒ...")
                    shuffled_quiz, shuffle_info = quality_checker.shuffle_quiz_answers(
                        quiz_data
                    )

                    # é‡æ–°åˆ†æéšæœºåŒ–åçš„åˆ†å¸ƒ
                    new_analysis = quality_checker.analyze_answer_distribution(
                        shuffled_quiz
                    )

                    # ä½¿ç”¨éšæœºåŒ–åçš„æ•°æ®
                    quiz_data = shuffled_quiz
                    analysis_result = new_analysis
                    
                    new_quality_score = new_analysis.get('quality_score', 0)
                    print(f"âœ… ç­”æ¡ˆåˆ†å¸ƒä¼˜åŒ–å®Œæˆï¼Œæ–°è´¨é‡åˆ†æ•°: {new_quality_score:.1f}/100")

            # å°†å¤„ç†åçš„æ•°æ®è½¬æ¢å›YAMLæ ¼å¼
            quiz_content = yaml.dump(
                quiz_data, default_flow_style=False,
                allow_unicode=True, sort_keys=False
            )

        except Exception as e:
            print(f"âš ï¸  è´¨é‡æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜: {e}")
            # é™é»˜å¤„ç†è´¨é‡æ£€æŸ¥é”™è¯¯ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
            quiz_content = yaml.dump(
                quiz_data, default_flow_style=False,
                allow_unicode=True, sort_keys=False
            )

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(quiz_file, 'w', encoding='utf-8') as f:
            f.write(quiz_content)

        print(f"âœ… æˆåŠŸ: '{concept}' çš„ {len(quiz_data)} é“æµ‹éªŒé¢˜å·²ç”Ÿæˆåœ¨ {quiz_file}")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆæµ‹éªŒæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        raise


def generate_quiz(
    concept: str,
    num_questions: int = typer.Option(
        None,
        "--num-questions",
        "-n",
        help="æŒ‡å®šé¢˜ç›®æ•°é‡ï¼ˆé»˜è®¤ä¸ºæ™ºèƒ½åˆ†æï¼‰",
        min=3,
        max=50
    ),
    mode: str = typer.Option(
        "auto",
        "--mode",
        help="ç”Ÿæˆæ¨¡å¼ï¼šautoï¼ˆæ™ºèƒ½åˆ†æï¼‰æˆ– fixedï¼ˆå›ºå®šæ¨¡å¼ï¼‰"
    ),
    max_tokens: int = typer.Option(
        8192,  # chatæ¨¡å‹é»˜è®¤4Kï¼Œæœ€å¤§8K
        "--max-tokens",
        help="æœ€å¤§è¾“å‡ºé•¿åº¦ï¼ˆé»˜è®¤8Kï¼Œchatæ¨¡å‹æœ€å¤§8Kï¼‰",
        min=1000,
        max=8192
    )
):
    """
    åŸºäºè§£é‡Šæ–‡æ¡£ç”Ÿæˆæµ‹éªŒé¢˜ç›®

    Args:
        concept: è¦ç”Ÿæˆæµ‹éªŒçš„æ¦‚å¿µåç§°
        num_questions: é¢˜ç›®æ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ™ºèƒ½åˆ†æï¼‰
        mode: ç”Ÿæˆæ¨¡å¼ (auto/fixedï¼Œé»˜è®¤auto)
        max_tokens: æœ€å¤§è¾“å‡ºé•¿åº¦ï¼ˆé»˜è®¤8Kï¼Œchatæ¨¡å‹æœ€å¤§8Kï¼‰
    """
    # è°ƒç”¨å†…éƒ¨ç‰ˆæœ¬ï¼Œé¿å…typer.Optionåºåˆ—åŒ–é—®é¢˜
    return generate_quiz_internal(
        concept=concept,
        num_questions=num_questions,
        mode=mode,
        max_tokens=max_tokens
    )
